
Installation instructions:
https://twm.me/how-to-install-llama2-linux-ubuntu/

nvidia-smi
Thu Nov  2 22:10:57 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3060        On  | 00000000:03:00.0 Off |                  N/A |
|  0%   48C    P8              15W / 170W |    282MiB / 12288MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A       992      G   /usr/lib/xorg/Xorg                          234MiB |
|    0   N/A  N/A      1252      G   /usr/bin/gnome-shell                         40MiB |
+---------------------------------------------------------------------------------------+


++++++++++++++++++++++++++++++++++++++++++++++++++++++
download email from Meta:

	

Llama 2 commercial license

You’re all set to start building with Llama 2.
The models listed below are now available to you as a commercial license holder. By downloading a model, you are agreeing to the terms and conditions of the license, acceptable use policy and Meta’s privacy policy.

Model weights available:

    Llama-2-7b
    Llama-2-7b-chat
    Llama-2-13b
    Llama-2-13b-chat
    Llama-2-70b
    Llama-2-70b-chat

With each model download, you’ll receive a copy of the License and Acceptable Use Policy, and can find all other information on the model and code on GitHub.

How to download the models:

    Visit the Llama repository in GitHub and follow the instructions in the README to run the download.sh script.
    When asked for your unique custom URL, please insert the following:
    https://download.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiYWV2MndjbDhxeXRxbzU1MTM1MW12aW5uIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5ODYyOTEwMX19fV19&Signature=o1DrV9JvKVI%7Enh5K9QaN1gVzm7hxXeOVHz0mUw7EMAXzc1CHwna01fdcivFKo4DOG-Sc8nxAB5OzyfXlcZblG2ffwsjOD3C%7EzpI93PSrGC3ppEud299OYxE1lOPmBVsYkMQncrj7SaN6IDpbkEuFvihS-48sp2jvKh557OQCu82J2bI6q7gG7t1q-jptVHyFY4NrENnED81618N9ifjZ6dBqVWeJgZahcHZivUY78xlp5ge8iE3pQ16J86tHZHzhrSUumUtGVPMIwMulFzm8FipL%7Eyhv82yWTknndSHv1hGbfNeKLTmKXetD%7EWc9pugGHnfMV5V%7Eb5dqgLYao34wAg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=3799707046975529
    Select which model weights to download

The unique custom URL provided will remain valid for model downloads for 24 hours, and requests can be submitted multiple times.
Now you’re ready to start building with Llama 2.

Helpful tips:

Please read the instructions in the GitHub repo and use the provided code examples to understand how to best interact with the models. In particular, for the fine-tuned chat models you must use appropriate formatting and correct system/instruction tokens to get the best results from the model.
You can find additional information about how to responsibly deploy Llama models in our Responsible Use Guide.

If you need to report issues:

If you or any Llama 2 user becomes aware of any violation of our license or acceptable use policies - or any bug or issues with Llama 2 that could lead to any such violations - please report it through one of the following means:

    Reporting issues with the model: Llama GitHub
    Giving feedback about potentially problematic output generated by the model: Llama output feedback
    Reporting bugs and security concerns: Bug Bounty Program
    Reporting violations of the Acceptable Use Policy: LlamaUseReport@meta.com

Subscribe to get the latest updates on Llama and Meta AI.

Meta’s GenAI Team

++++++++++++++++++++++++++++++++++++++++++++++++++++++

Llama2 directory structure:
daryl@daryl-old-pc:~/Documents/3060_GPU_PC/llama2/llama-main$ ls -l
total 3202000
-rw-rw-r-- 1 daryl daryl       3536 Oct 18 11:38 CODE_OF_CONDUCT.md
-rw-rw-r-- 1 daryl daryl       1236 Oct 18 11:38 CONTRIBUTING.md
-rw-rw-r-- 1 daryl daryl 3276992138 Oct 16 01:09 cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb
-rwxr-xr-x 1 daryl daryl       2515 Oct 18 11:38 download.sh
-rw-rw-r-- 1 daryl daryl       4602 Oct 18 11:38 example_chat_completion.py
-rwxr-xr-x 1 daryl daryl       2443 Oct 18 11:38 example_text_completion.py
-rw-rw-r-- 1 daryl daryl       6356 Oct 18 11:38 FAQ.md
-rw-rw-r-- 1 daryl daryl       7023 Oct 18 11:38 LICENSE
drwxrwxr-x 3 daryl daryl       4096 Nov  2 21:44 llama
drwxrwxr-x 2 daryl daryl       4096 Oct 28 19:52 llama-2-13b
drwxrwxr-x 2 daryl daryl       4096 Oct 28 19:34 llama-2-7b
-rw-rw-r-- 1 daryl daryl       7613 Oct 18 11:38 MODEL_CARD.md
-rw-rw-r-- 1 daryl daryl          0 Oct 28 19:25 params.json
-rwxr-xr-x 1 daryl daryl       7856 Oct 18 11:38 README.md
-rwxr-xr-x 1 daryl daryl         35 Oct 18 11:38 requirements.txt
-rw-rw-r-- 1 daryl daryl    1253223 Oct 18 11:38 Responsible-Use-Guide.pdf
-rwxr-xr-x 1 daryl daryl        426 Oct 18 11:38 setup.py
-rw-rw-r-- 1 daryl daryl         50 Jul 13 16:27 tokenizer_checklist.chk
-rw-rw-r-- 1 daryl daryl     499723 Jul 13 16:27 tokenizer.model
-rw-rw-r-- 1 daryl daryl       1930 Oct 18 11:38 UPDATES.md
-rw-rw-r-- 1 daryl daryl       4766 Oct 18 11:38 USE_POLICY.md

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Contents of the llama-2-7b directory
total 13161072
-rw-rw-r-- 1 daryl daryl         100 Jul 13 17:00 checklist.chk
-rw-rw-r-- 1 daryl daryl 13476925163 Jul 13 17:00 consolidated.00.pth
-rw-rw-r-- 1 daryl daryl         102 Jul 13 17:00 params.json

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


Ultimately, this was the failure that I wanted to record. The smallest version of Llama2 is 7 billion parameters. My GPU, Nvidia 3060 with 12 GB memory did not have enough memory needed for the smallest version of LLAMA2>

daryl@daryl-old-pc:~/Documents/3060_GPU_PC/llama2/llama-main$ python script_name.py --max_batch_size=1 --ckpt_dir=path_to_checkpoint_directory --tokenizer_path=path_to_tokenizer_file
python3 example_chat_completion.py --ckpt_dir=/home/daryl/Documents/3060_GPU_PC/llama2/llama-main/llama-2-7b --tokenizer_path=/home/daryl/Documents/3060_GPU_PC/llama2/llama-main/tokenizer.model
Command 'python' not found, did you mean:
  command 'python3' from deb python3
  command 'python' from deb python-is-python3
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
/home/daryl/.local/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)
  _C._set_default_tensor_type(t)
Traceback (most recent call last):
  File "/home/daryl/Documents/3060_GPU_PC/llama2/llama-main/example_chat_completion.py", line 104, in <module>
    fire.Fire(main)
  File "/home/daryl/.local/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/daryl/.local/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/daryl/.local/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/daryl/Documents/3060_GPU_PC/llama2/llama-main/example_chat_completion.py", line 35, in main
    generator = Llama.build(
  File "/home/daryl/Documents/3060_GPU_PC/llama2/llama-main/llama/generation.py", line 119, in build
    model = Transformer(model_args)
  File "/home/daryl/Documents/3060_GPU_PC/llama2/llama-main/llama/model.py", line 443, in __init__
    self.layers.append(TransformerBlock(layer_id, params))
  File "/home/daryl/Documents/3060_GPU_PC/llama2/llama-main/llama/model.py", line 375, in __init__
    self.attention = Attention(args)
  File "/home/daryl/Documents/3060_GPU_PC/llama2/llama-main/llama/model.py", line 236, in __init__
    self.cache_k = torch.zeros(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 11.76 GiB of which 6.38 MiB is free. Including non-PyTorch memory, this process has 11.47 GiB memory in use. Of the allocated memory 11.36 GiB is allocated by PyTorch, and 1.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
